import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.text_splitter import RecursiveCharacterTextSplitter

from typing import Any, List, Optional
import psutil  # æ·»åŠ psutilåº“

# å‘é‡æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='./')

# æºå¤§æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
model_path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰å‘é‡æ¨¡å‹è·¯å¾„
embedding_model_path = './AI-ModelScope/bge-small-zh-v1___5'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100

# å®šä¹‰æºå¤§æ¨¡å‹ç±»
class Yuan2_LLM(LLM):
    """
    class for Yuan2_LLM
    """
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, mode_path :str):
        super().__init__()

        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
        print("Creat tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(mode_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

        print("Creat model...")
        self.model = AutoModelForCausalLM.from_pretrained(mode_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        prompt = prompt.strip()
        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = self.model.generate(inputs,do_sample=False,max_new_tokens=4096)
        output = self.tokenizer.decode(outputs[0])
        response = output.split("<sep>")[-1].split("<eod>")[0]

        return response

    @property
    def _llm_type(self) -> str:
        return "Yuan2_LLM"

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–llmå’Œembeddings
@st.cache_resource
def get_models():
    llm = Yuan2_LLM(model_path)

    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    return llm, embeddings

summarizer_template = """
å‡è®¾ä½ æ˜¯ä¸€ä¸ªåè‘—é˜…è¯»åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸€æ®µè¯æ¦‚æ‹¬ä¸‹é¢åè‘—çš„ä¸»è¦å†…å®¹ï¼Œ200å­—å·¦å³ã€‚

{text}
"""

# å®šä¹‰Summarizerç±»
class Summarizer:
    """
    class for Summarizer.
    """

    def __init__(self, llm):
        self.llm = llm
        self.prompt = PromptTemplate(
            input_variables=["text"],
            template=summarizer_template
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def summarize(self, docs):
        # æ‹¼æ¥æ‰€æœ‰é¡µé¢çš„å†…å®¹ç”Ÿæˆæ‘˜è¦
        content = ' '.join([doc.page_content for doc in docs])

        summary = self.chain.run(content)
        return summary

chatbot_template  = '''
å‡è®¾ä½ æ˜¯ä¸€ä¸ªåè‘—é˜…è¯»åŠ©æ‰‹ï¼Œè¯·åŸºäºèƒŒæ™¯ï¼Œç®€è¦å›ç­”é—®é¢˜ã€‚

èƒŒæ™¯ï¼š
{context}

é—®é¢˜ï¼š
{question}
'''.strip()

# å®šä¹‰ChatBotç±»
class ChatBot:
    """
    class for ChatBot.
    """

    def __init__(self, llm, embeddings):
        self.prompt = PromptTemplate(
            input_variables=["text"],
            template=chatbot_template
        )
        self.chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=self.prompt)
        self.embeddings = embeddings

        # åŠ è½½ text_splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=450,
            chunk_overlap=10,
            length_function=len
        )

    def run(self, docs, query):
        # è¯»å–æ‰€æœ‰å†…å®¹
        text = ''.join([doc.page_content for doc in docs])

        # åˆ‡åˆ†æˆchunks
        all_chunks = self.text_splitter.split_text(text=text)

        # è½¬æˆå‘é‡å¹¶å­˜å‚¨
        VectorStore = FAISS.from_texts(all_chunks, embedding=self.embeddings)

        # æ£€ç´¢ç›¸ä¼¼çš„chunks
        chunks = VectorStore.similarity_search(query=query, k=1)

        # ç”Ÿæˆå›å¤
        response = self.chain.run(input_documents=chunks, question=query)

        return chunks, response

def main():
    # åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
    st.title('ğŸ’¬ Yuan2.0 AIReader')

    # è·å–llmå’Œembeddings
    llm, embeddings = get_models()

    # åˆå§‹åŒ–summarizer
    summarizer = Summarizer(llm)

    # åˆå§‹åŒ–ChatBot
    chatbot = ChatBot(llm, embeddings)

    # åˆ›å»ºCPUå’Œå†…å­˜å ç”¨çš„æ˜¾ç¤ºçª—
    st.sidebar.header("System Resources")
    cpu_usage = psutil.cpu_percent(interval=1)
    memory_info = psutil.virtual_memory()
    st.sidebar.text(f"CPU Usage: {cpu_usage}%")
    st.sidebar.text(f"Memory Usage: {memory_info.percent}% ({memory_info.used // (1024**2)}MB / {memory_info.total // (1024**2)}MB)")

    # å¦‚æœæœ‰GPUï¼Œæ˜¾ç¤ºGPUå’Œæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        gpu_usage = torch.cuda.utilization(0)
        gpu_memory = torch.cuda.memory_allocated(0) / (1024**2)  # è½¬æ¢ä¸ºMB
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**2)  # æ€»æ˜¾å­˜ï¼Œè½¬æ¢ä¸ºMB
        st.sidebar.text(f"GPU Usage: {gpu_usage}%")
        st.sidebar.text(f"GPU Memory Usage: {gpu_memory:.2f}MB / {gpu_memory_total:.2f}MB")

    # ä¸Šä¼ pdf
    uploaded_file = st.file_uploader("Upload your file", type=['pdf', 'txt'])

    if uploaded_file:
        # åŠ è½½ä¸Šä¼ PDFçš„å†…å®¹
        file_content = uploaded_file.read()

        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = uploaded_file.name.split('.')[-1]

        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        temp_file_path = f"temp.{file_extension}"
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)

        # åŠ è½½ä¸´æ—¶æ–‡ä»¶ä¸­çš„å†…å®¹
        if file_extension == 'pdf':
            loader = PyPDFLoader(temp_file_path)
        elif file_extension == 'txt':
            loader = TextLoader(temp_file_path)
        docs = loader.load()

        st.chat_message("assistant").write(f"æ­£åœ¨ç”Ÿæˆåè‘—æ¦‚æ‹¬")

        # ç”Ÿæˆæ¦‚æ‹¬
        try:
            summary = summarizer.summarize(docs)
        except Exception as e:
            st.error(f"Error during summarization: {e}")
        
        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(summary)

        # æ¥æ”¶ç”¨æˆ·é—®é¢˜
        if query := st.text_input("Ask questions about your file"):

            # æ£€ç´¢ + ç”Ÿæˆå›å¤
            chunks, response = chatbot.run(docs, query)

            # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
            st.chat_message("assistant").write(f"æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯")
            st.chat_message("assistant").write(chunks)

            st.chat_message("assistant").write(f"æ­£åœ¨ç”Ÿæˆå›å¤")
            st.chat_message("assistant").write(response)

if __name__ == '__main__':
    main()
