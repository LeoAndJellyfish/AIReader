import streamlit as st
import json
import torch
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import TextLoader
from transformers import pipeline as hf_pipeline
 
# è®¾ç½®æ ‡é¢˜å’Œæè¿°
st.title("ğŸ’¬ Yuan2.0 AIReader")
st.write("ä¸€ä¸ªç»“åˆäº† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„åè‘—é˜…è¯»åŠ©æ‰‹ã€‚")
 
# åŠ è½½åè‘—æ¸…å• JSON æ–‡ä»¶
@st.cache_data
def load_books():
    # ä»JSONæ–‡ä»¶ä¸­åŠ è½½åè‘—æ¸…å•
    try:
        with open('./code/books.json', 'r', encoding='utf-8') as f:
            return json.load(f)["books"]
    except Exception as e:
        st.error(f"åŠ è½½åè‘—æ¸…å•æ—¶å‡ºé”™: {e}")
        return []
 
books = load_books()
 
# åè‘—é€‰æ‹©æ¡†
book_names = [book["name"] for book in books]
book_selection = st.selectbox("è¯·é€‰æ‹©ä½ æƒ³æé—®çš„åè‘—ï¼š", book_names)
 
# æ ¹æ®é€‰æ‹©çš„åè‘—è·å–å¯¹åº”çš„ document_path
def get_document_path(selected_book_name):
    # æ ¹æ®åè‘—åç§°æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£è·¯å¾„
    for book in books:
        if book["name"] == selected_book_name:
            return book["document_path"]
    st.error("æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£è·¯å¾„")
    return None
 
document_path = get_document_path(book_selection)
 
# å®šä¹‰ HuggingFacePipeline ä»¥é€‚é…æ¨¡å‹
@st.cache_resource
def load_pipeline(model_path: str, torch_dtype):
    # åŠ è½½HuggingFaceæ¨¡å‹ç®¡é“
    try:
        st.write("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        hf_model_pipeline = hf_pipeline("text-generation", model=model_path, tokenizer=model_path, torch_dtype=torch_dtype, device=0)
        st.write("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return HuggingFacePipeline(pipeline=hf_model_pipeline)
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None
 
# åŠ è½½ HuggingFacePipeline
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
torch_dtype = torch.bfloat16  # A10
pipeline = load_pipeline(model_path, torch_dtype)
 
# å®šä¹‰å‘é‡åº“ç´¢å¼•
@st.cache_resource
def load_vectorstore(document_path: str, embed_model_path: str):
    # åŠ è½½æ–‡æœ¬æ–‡ä»¶å¹¶åˆ›å»ºå‘é‡åº“
    loader = TextLoader(document_path)
    embeddings = HuggingFaceEmbeddings(embed_model_path)
    return FAISS.from_documents(loader.load(), embeddings)
 
# åŠ è½½å‘é‡åº“
embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
vectorstore = load_vectorstore(document_path, embed_model_path) if document_path else None
 
# å®šä¹‰ RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=pipeline,
    chain_type="stuff",
    retriever=vectorstore.as_retriever() if vectorstore else None
) if vectorstore else None
 
# åˆå§‹åŒ– session_state
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if 'previous_book' not in st.session_state:
    st.session_state.previous_book = ""
 
# æ¸…ç©ºå¯¹è¯å†å²å½“åè‘—å‘ç”Ÿå˜åŒ–æ—¶
if st.session_state.previous_book != book_selection:
    st.session_state["messages"] = []
    st.session_state.previous_book = book_selection
 
# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
 
# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    try:
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
 
        # ä½¿ç”¨ RAG Chain è¿›è¡Œé—®ç­”
        if qa_chain:
            response = qa_chain.run({"query": prompt})
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)
        else:
            st.error("æ— æ³•è¿›è¡Œé—®ç­”ï¼Œè¯·æ£€æŸ¥å‘é‡åº“æ˜¯å¦æ­£ç¡®åŠ è½½ã€‚")
    except Exception as e:
        st.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")