import streamlit as st
import json
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import torch
from typing import List
import numpy as np
import faiss  # å¯¼å…¥FAISSåº“ç”¨äºé«˜æ•ˆçš„å‘é‡æ£€ç´¢

# è®¾ç½®æ ‡é¢˜å’Œæè¿°
st.title("ğŸ’¬ Yuan2.0 AIReader")
st.write("ä¸€ä¸ªç»“åˆäº† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„åè‘—é˜…è¯»åŠ©æ‰‹ã€‚")

# åŠ è½½åè‘—æ¸…å• JSON æ–‡ä»¶
def load_books():
    try:
        with open('./code/books.json', 'r', encoding='utf-8') as f:
            books_data = json.load(f)
        return books_data["books"]
    except Exception as e:
        st.error(f"åŠ è½½åè‘—æ¸…å•æ—¶å‡ºé”™: {e}")
        return []

books = load_books()

# åè‘—é€‰æ‹©æ¡†
book_names = [book["name"] for book in books]
book_selection = st.selectbox("è¯·é€‰æ‹©ä½ æƒ³æé—®çš„åè‘—ï¼š", book_names)

# æ ¹æ®é€‰æ‹©çš„åè‘—è·å–å¯¹åº”çš„ document_path
def get_document_path(selected_book_name):
    for book in books:
        if book["name"] == selected_book_name:
            return book["document_path"]
    st.error("æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£è·¯å¾„")
    return None

document_path = get_document_path(book_selection)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model_and_tokenizer():
    try:
        st.write("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                              '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                              '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
        st.write("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return tokenizer, model
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None, None

# åŠ è½½æ¨¡å‹å’Œtokenizer
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
torch_dtype = torch.bfloat16 # ä½¿ç”¨A10çš„torch_bfloat16æ•°æ®ç±»å‹
tokenizer, model = load_model_and_tokenizer()

# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    def __init__(self, path: str) -> None:
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(path)
            self.model = AutoModel.from_pretrained(path).cuda()
        except Exception as e:
            st.error(f"åŠ è½½Embeddingæ¨¡å‹æ—¶å‡ºé”™: {e}")
            raise

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        try:
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                sentence_embeddings = model_output[0][:, 0]
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            return sentence_embeddings.cpu().tolist()
        except Exception as e:
            st.error(f"è·å–åµŒå…¥æ—¶å‡ºé”™: {e}")
            return []

# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:
        try:
            self.documents = []
            # ä½¿ç”¨ç”Ÿæˆå™¨é€è¡Œè¯»å–æ–‡æ¡£ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡æ¡£
            with open(document_path, 'r', encoding='utf-8') as f:
                for line in f:
                    self.documents.append(line.strip())
            self.embed_model = embed_model
            # è·å–æ–‡æ¡£çš„å‘é‡è¡¨ç¤º
            self.vectors = self.embed_model.get_embeddings(self.documents)

            # ä½¿ç”¨FAISSåº“æ„å»ºå‘é‡ç´¢å¼•
            dimension = len(self.vectors[0])
            self.index = faiss.IndexFlatL2(dimension)
            self.index.add(np.array(self.vectors, dtype=np.float32))
        except Exception as e:
            st.error(f"åˆå§‹åŒ–VectorStoreIndexæ—¶å‡ºé”™: {e}")
            raise

    def query(self, question: str, k: int = 1) -> List[str]:
        try:
            # è·å–é—®é¢˜çš„å‘é‡è¡¨ç¤º
            question_vector = self.embed_model.get_embeddings([question])[0]
            question_vector = np.array(question_vector, dtype=np.float32).reshape(1, -1)
            # ä½¿ç”¨FAISSåº“è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
            distances, indices = self.index.search(question_vector, k)
            # è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£
            return [self.documents[i] for i in indices[0]]
        except Exception as e:
            st.error(f"æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return []

# å®ä¾‹åŒ– EmbeddingModel å’Œ VectorStoreIndex
embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
embed_model = EmbeddingModel(embed_model_path)
index = VectorStoreIndex(document_path, embed_model)

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    try:
        # å¦‚æœç”¨æˆ·åˆ‡æ¢åè‘—ï¼Œåˆ™æ¸…ç©ºå¯¹è¯å†å²
        if document_path != get_document_path(book_selection):
            st.session_state["messages"] = []
            document_path = get_document_path(book_selection)
            index = VectorStoreIndex(document_path, embed_model)
        
        # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "user", "content": prompt})

        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
        st.chat_message("user").write(prompt)

        # ä½¿ç”¨ç´¢å¼•æŸ¥è¯¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡
        context = index.query(prompt)

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        if context:
            full_prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{prompt}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚<sep>'
        else:
            full_prompt = prompt + "<sep>"

        inputs = tokenizer(full_prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = model.generate(
            inputs,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            max_length=1024
        )
        response = tokenizer.decode(outputs[0]).split("<sep>")[-1].replace("<eod>", '')

        # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "assistant", "content": response})

        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(response)
    except Exception as e:
        st.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")
