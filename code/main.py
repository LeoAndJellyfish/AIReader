import os
import streamlit as st
import json
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import torch
from typing import List
import numpy as np
import pickle
import faiss

# è®¾ç½®æ ‡é¢˜å’Œæè¿°
st.title("ğŸ’¬ Yuan2.0 AIReader")
st.write("ä¸€ä¸ªç»“åˆäº† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„åè‘—é˜…è¯»åŠ©æ‰‹ã€‚")

# åŠ è½½åè‘—æ¸…å• JSON æ–‡ä»¶
def load_books():
    try:
        with open('./code/books.json', 'r', encoding='utf-8') as f:
            books_data = json.load(f)
        return books_data["books"]
    except Exception as e:
        st.error(f"åŠ è½½åè‘—æ¸…å•æ—¶å‡ºé”™: {e}")
        return []

books = load_books()

# åè‘—é€‰æ‹©æ¡†
book_names = [book["name"] for book in books]
book_selection = st.selectbox("è¯·é€‰æ‹©ä½ æƒ³æé—®çš„åè‘—ï¼š", book_names)

# æ ¹æ®é€‰æ‹©çš„åè‘—è·å–å¯¹åº”çš„ document_path
def get_document_path(selected_book_name):
    for book in books:
        if book["name"] == selected_book_name:
            return book["document_path"]
    st.error("æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£è·¯å¾„")
    return None

document_path = get_document_path(book_selection)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model_and_tokenizer():
    try:
        st.session_state.loading_message = st.empty()
        st.session_state.loading_message.write("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                              '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                              '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

        st.session_state.tokenizer = tokenizer
        st.session_state.model = model

        st.session_state.loading_message.empty()
        st.session_state.model_loaded = True
        st.write("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return tokenizer, model
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None, None

# åŠ è½½æ¨¡å‹å’Œtokenizer
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
torch_dtype = torch.bfloat16  # A10
if 'model_loaded' not in st.session_state:
    tokenizer, model = load_model_and_tokenizer()
else:
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model

# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    def __init__(self, path: str) -> None:
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(path)
            self.model = AutoModel.from_pretrained(path).cuda()
        except Exception as e:
            st.error(f"åŠ è½½Embeddingæ¨¡å‹æ—¶å‡ºé”™: {e}")
            raise

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        try:
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                sentence_embeddings = model_output[0][:, 0]
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            return sentence_embeddings.cpu().tolist()
        except Exception as e:
            st.error(f"è·å–åµŒå…¥æ—¶å‡ºé”™: {e}")
            return []

# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    def __init__(self, document_path: str, embed_model: EmbeddingModel, batch_size: int = 32) -> None:
        self.document_path = document_path
        self.embed_model = embed_model
        self.batch_size = batch_size
        self.vector_cache_path = f"{document_path}.pkl"
        self.documents = []
        self.vectors = []
        self.index = None
        self.load_or_create_vectors()

    def load_or_create_vectors(self):
        if os.path.exists(self.vector_cache_path):
            with open(self.vector_cache_path, 'rb') as f:
                self.vectors = pickle.load(f)
            self.documents = [line.strip() for line in open(self.document_path, 'r', encoding='utf-8')]
            self.build_faiss_index()
        else:
            self.documents = [line.strip() for line in open(self.document_path, 'r', encoding='utf-8')]
            self.vectors = self.load_vectors_in_batches()
            with open(self.vector_cache_path, 'wb') as f:
                pickle.dump(self.vectors, f)
            self.build_faiss_index()

    def load_vectors_in_batches(self) -> List[List[float]]:
        vectors = []
        num_batches = (len(self.documents) + self.batch_size - 1) // self.batch_size

        progress_bar = st.progress(0)
        
        for i in range(num_batches):
            batch_docs = self.documents[i * self.batch_size:(i + 1) * self.batch_size]
            batch_vectors = self.embed_model.get_embeddings(batch_docs)
            vectors.extend(batch_vectors)
            
            progress_bar.progress((i + 1) / num_batches)
        
        progress_bar.empty()
        return vectors

    def build_faiss_index(self):
        if not self.vectors:
            st.error("æ²¡æœ‰å‘é‡æ•°æ®æ¥æ„å»ºFaissç´¢å¼•")
            return
        
        dimension = len(self.vectors[0])  # å‘é‡çš„ç»´åº¦
        self.index = faiss.IndexFlatL2(dimension)  # ä½¿ç”¨ L2 è·ç¦»
        self.index.add(np.array(self.vectors, dtype=np.float32))

    def query(self, question: str, k: int = 1) -> List[str]:
        try:
            question_vector = self.embed_model.get_embeddings([question])[0]
            question_vector = np.array(question_vector, dtype=np.float32).reshape(1, -1)
            _, indices = self.index.search(question_vector, k)
            return [self.documents[i] for i in indices[0]]
        except Exception as e:
            st.error(f"æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return []

# æ¯æ¬¡ç”¨æˆ·é€‰æ‹©åè‘—æ—¶ï¼ŒåŠ è½½å¯¹åº”çš„ knowledge æ–‡æ¡£
embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
embed_model = EmbeddingModel(embed_model_path)

if document_path:
    index = VectorStoreIndex(document_path, embed_model, batch_size=32)

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# åˆå§‹åŒ– previous_book
if 'previous_book' not in st.session_state:
    st.session_state.previous_book = ""

# æ¸…ç©ºå¯¹è¯å†å²å½“åè‘—å‘ç”Ÿå˜åŒ–æ—¶
if st.session_state.previous_book != book_selection:
    st.session_state["messages"] = []
    st.session_state.previous_book = book_selection

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    try:
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        # ä½¿ç”¨ç´¢å¼•æŸ¥è¯¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡
        context = index.query(prompt) if index else []

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        if context:
            full_prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªåè‘—é˜…è¯»åŠ©æ‰‹ï¼Œä½ èƒ½å¤Ÿæ ¹æ®æä¾›çš„åŸæ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                åŸæ–‡ï¼š{context}
                é—®é¢˜ï¼š{prompt}
                è¯·å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
                <sep>
                """
        else:
            full_prompt = prompt + "<sep>"

        inputs = tokenizer(full_prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = model.generate(
            inputs,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            max_length=1024
        )
        response = tokenizer.decode(outputs[0]).split("<sep>")[-1].replace("<eod>", '')

        st.session_state.messages.append({"role": "assistant", "content": response})
        st.chat_message("assistant").write(response)
    except Exception as e:
        st.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")
