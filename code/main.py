import streamlit as st
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import torch
from typing import List
import numpy as np

# è®¾ç½®æ ‡é¢˜å’Œæè¿°
st.title("ğŸ’¬ Yuan2.0 AIReader")
st.write("ä¸€ä¸ªç»“åˆäº† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„åè‘—é˜…è¯»åŠ©æ‰‹ã€‚")
 
# å®šä¹‰æ¨¡å‹è·¯å¾„å’Œæ•°æ®ç±»å‹
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
torch_dtype = torch.bfloat16 # A10
 
# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    def __init__(self, path: str) -> None:
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(path)
            self.model = AutoModel.from_pretrained(path).cuda()
        except Exception as e:
            st.error(f"åŠ è½½Embeddingæ¨¡å‹æ—¶å‡ºé”™: {e}")
            raise
     
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        try:
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                sentence_embeddings = model_output[0][:, 0]
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            return sentence_embeddings.cpu().tolist()
        except Exception as e:
            st.error(f"è·å–åµŒå…¥æ—¶å‡ºé”™: {e}")
            return []
 
# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:
        try:
            self.documents = []
            for line in open(document_path, 'r', encoding='utf-8'):
                self.documents.append(line.strip())
            self.embed_model = embed_model
            self.vectors = self.embed_model.get_embeddings(self.documents)
        except Exception as e:
            st.error(f"åˆå§‹åŒ–VectorStoreIndexæ—¶å‡ºé”™: {e}")
            raise
     
    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        try:
            dot_product = np.dot(vector1, vector2)
            magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
            return dot_product / magnitude if magnitude else 0
        except Exception as e:
            st.error(f"è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0
     
    def query(self, question: str, k: int = 1) -> List[str]:
        try:
            question_vector = self.embed_model.get_embeddings([question])[0]
            result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])
            return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist()
        except Exception as e:
            st.error(f"æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return []
 
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model():
    try:
        st.write("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                              '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                              '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)
 
        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
         
        # åŠ è½½Embeddingæ¨¡å‹
        embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
        embed_model = EmbeddingModel(embed_model_path)
 
        # åˆ›å»ºæ–‡æ¡£ç´¢å¼•
        document_path = './code/knowledge.txt'
        index = VectorStoreIndex(document_path, embed_model)
 
        st.write("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return tokenizer, model, index
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None, None, None
 
# åŠ è½½æ¨¡å‹å’Œtokenizer
tokenizer, model, index = load_model()
 
# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []
 
# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
 
# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    try:
        # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "user", "content": prompt})
 
        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
        st.chat_message("user").write(prompt)
 
        # ä½¿ç”¨ç´¢å¼•æŸ¥è¯¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡
        context = index.query(prompt)
         
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        if context:
            full_prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{prompt}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚<sep>'
        else:
            full_prompt = prompt + "<sep>"
 
        inputs = tokenizer(full_prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = model.generate(
            inputs, 
            do_sample=True, 
            temperature=0.7, 
            top_k=50, 
            top_p=0.95, 
            max_length=1024
        )
        response = tokenizer.decode(outputs[0]).split("<sep>")[-1].replace("<eod>", '')
 
        # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "assistant", "content": response})
 
        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(response)
    except Exception as e:
        st.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")